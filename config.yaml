# ==== config.yaml (small-data, ready-to-train; for scripts/train.py) ====

# 変数（$... は subs_vars で展開）
variables:
  $studyname: AllPubchem_test
  $seed: 111
  $lr: 0.001
  $optimizer: adamw         # デフォルト最適化手法
  $pad_token: 0
  $start_token: 1
  $end_token: 2
  $voc_size: 129
  $PROJECT_DIR: .
  $DATA_DIR: data

# ログ（train.py は小文字で渡すのでこのままでOK）
logging:
  stream_level: info
  file_level: debug

# 出力ディレクトリ（train.py の仕様に合わせて top-level に）
result_dir:
  dirname: "./result/$TIMESTAMP_$studyname"
  duplicate: "merge"        # 既存があっても追記/統合

# モデル定義（あなたの元設定の modules をそのまま移植）
model:
  modules:
    teacherforcer: { type: TeacherForcer, length_dim: 1 }
    masker:        { type: MaskMaker, mask_token: 0, dtype: bool, direction: equal }
    enc_embedding:
      type: PositionalEmbedding
      embedding: { num_embeddings: 1123, embedding_dim: 512, padding_idx: 0 }
      dropout: 0.0
      max_len: 250
    encoder:
      type: TransformerEncoder
      layer:
        d_model: 512
        nhead: 8
        d_ff_factor: 4
        dropout: 0.0
        activation: newgelu
        layer_norm_eps: 1.0e-09
      n_layer: 8
      init:
        self_attn.in_proj_weight: glorot_uniform
        self_attn.in_proj_bias: zero
        self_attn.out_proj.weight: glorot_uniform
        self_attn.out_proj.bias: zero
        linear1.weight: { type: normal, mean: 0.0, std: 0.02 }
        linear1.bias: zero
        linear2.weight: { type: normal, mean: 0.0, std: 0.02 }
        linear2.bias: zero
    pooler:
      type: NoAffinePooler
      input_size: [max_len, batch_size, 512]
    latent2mu:
      type: Tunnel
      input_size: [batch_size, 1536]
      layers:
        - { type: linear, size: 512, init: { bias: zero } }
    latent2var:
      type: Tunnel
      input_size: [batch_size, 1536]
      layers:
        - { type: linear, size: 512, init: { bias: zero } }
        - { type: function, function: { type: softplus } }
    vae: { type: VAE }
    latent2dec:
      type: Tunnel
      input_size: [batch_size, 512]
      layers:
        - { type: affine, weight: 10.0, bias: 0.0 }
        - { type: linear, size: 512, init: { bias: zero } }
    dec_embedding:
      type: PositionalEmbedding
      embedding: { num_embeddings: 129, embedding_dim: 512, padding_idx: 0 }
      dropout: 0.0
      max_len: 250
    decoder:
      type: AttentionDecoder
      max_len: 250
      layer:
        d_model: 512
        nhead: 8
        dropout: 0.0
        layer_norm_eps: 1.0e-09
        activation: newgelu
        d_ff_factor: 4
      num_layers: 8
      init:
        self_attn.in_proj_weight: glorot_uniform
        self_attn.in_proj_bias: zero
        self_attn.out_proj.weight: glorot_uniform
        self_attn.out_proj.bias: zero
        linear1.weight: { type: normal, mean: 0.0, std: 0.02 }
        linear1.bias: zero
        linear2.weight: { type: normal, mean: 0.0, std: 0.02 }
        linear2.bias: zero
    dec2proba:
      type: Tunnel
      input_size: [batch_size, length, 512]
      layers:
        - { type: layernorm, args: { elementwise_affine: false } }
        - { type: linear, size: 129, init: { bias: zero } }
    dec_supporter:
      type: GreedyDecoder
      start_token: 1
    sequencece:
      type: CrossEntropyLoss
      reduction: sum
      ignore_index: 0
    -d_kl:         { type: MinusD_KLLoss }
    -d_kl_factor:  { type: Affine, weight: 0.001 }

# Optimizer / Scheduler（デフォルト入り）
optimizer:
  type: "$optimizer"   # ← variables から 'adamw'
  lr: $lr              # ← 0.001
  weight_decay: 0.01   # AdamW なので適度な正則化を付与
  betas: [0.9, 0.999]
  eps: 1.0e-8

# get_scheduler は 'linear' / 'multistep' / 'exponential' に加えて
# 'warmup' も LambdaLR でサポートされる実装です
scheduler:
  type: warmup
  warmup: 200          # 200 step ウォームアップ（小規模スモーク用）

# metrics（オリジナルを踏襲）
metrics:
  Perfect accuracy:
    type: perfect
    input: forced
    target: dec_target
    pad_token: $pad_token
  Partial accuracy (greedy decode):
    type: partial
    input: greedy
    target: dec_target
    pad_token: $pad_token
  Partial accuracy (teacher forcing):
    type: partial
    input: forced
    target: dec_target
    pad_token: $pad_token

# Hooks（hook_type2class に存在するもののみ）
pre_hooks:
  notice:
    type: notice_alarm
    studyname: "$studyname"
    alarm: { type: count, target: step, step: 2500, start: 2500 }
    end: true
  step_abort:
    type: step_abort
    threshold: 20000
  time_abort:
    type: time_abort
    threshold: 7200   # 秒（= 2時間）

# データ（NormalDataLoader 前提の最小構成）
data:
  train:
    type: normal
    seed: $seed
    batch_size: 64
    datasets:
      input:
        type: pkl
        path_list:
          - "$DATA_DIR/AllPubchem_test_random.pkl"
      target:
        type: pkl
        path_list:
          - "$DATA_DIR/AllPubchem_test_canonical.pkl"

  vals:
    - type: normal
      seed: $seed
      batch_size: 128
      datasets:
        input:
          type: pkl
          path_list:
            - "$DATA_DIR/AllPubchem_test_random.pkl"
        target:
          type: pkl
          path_list:
            - "$DATA_DIR/AllPubchem_test_canonical.pkl"

# トレーニング基本パラメータ
training:
  epochs: 1
  steps_per_epoch: 50    # 小規模検証なので短め
  grad_accum: 1
  save_every: 1
  process:
    type: forward        # get_process('forward') を呼ぶ

# 参考：BucketDataLoader を使いたい場合（ファイルが揃っているなら）
# data:
#   train:
#     type: bucket
#     seed: $seed
#     bucket_dset: input
#     batch_size: [64,64,64,64,48,48,32,32,24,24,16,16,8,8,8,8]
#     bins: [20,30,40,50,60,70,80,90,100,120,140,160,180,200,220]
#     datasets:
#       datasets:
#         input:
#           type: string
#           padding_value: 0
#           path_list: "$DATA_DIR/Pubchem_chunk_pro_0_ran.pkl"
#         target:
#           type: string
#           padding_value: 0
#           path_list: "$DATA_DIR/Pubchem_chunk_pro_0_can.pkl"
