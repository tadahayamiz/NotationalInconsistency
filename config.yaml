# ==== config.yaml（type名を CamelCase に統一；小データ実学習） ====

variables:
  $studyname: AllPubchem_test
  $seed: 111
  $lr: 0.001
  $optimizer: adamw
  $pad_token: 0
  $start_token: 1
  $end_token: 2
  $voc_size: 129
  $PROJECT_DIR: .
  $DATA_DIR: data

logging:
  stream_level: info
  file_level: debug

result_dir:
  dirname: "./result/$TIMESTAMP_$studyname"
  duplicate: "merge"

model:
  modules:
    teacherforcer: { type: TeacherForcer, length_dim: 1 }
    masker:        { type: MaskMaker, mask_token: 0, dtype: bool, direction: equal }
    enc_embedding:
      type: PositionalEmbedding
      embedding: { num_embeddings: 1123, embedding_dim: 512, padding_idx: 0 }
      dropout: 0.0
      max_len: 250
    encoder:
      type: TransformerEncoder
      layer:
        d_model: 512
        nhead: 8
        d_ff_factor: 4
      n_layer: 8
    pooler:
      type: NoAffinePooler
      input_size: [max_len, batch_size, 512]
    latent2mu:
      type: Tunnel
      input_size: [batch_size, 1536]
      layers:
        - { type: linear, size: 512, init: { bias: zero } }
    latent2var:
      type: Tunnel
      input_size: [batch_size, 1536]
      layers:
        - { type: linear, size: 512, init: { bias: zero } }
        - { type: function, function: { type: softplus } }
    vae: { type: VAE }
    latent2dec:
      type: Tunnel
      input_size: [batch_size, 512]
      layers:
        - { type: affine, weight: 10.0, bias: 0.0 }
        - { type: linear, size: 512, init: { bias: zero } }
    dec_embedding:
      type: PositionalEmbedding
      embedding: { num_embeddings: 129, embedding_dim: 512, padding_idx: 0 }
      dropout: 0.0
      max_len: 250
    decoder:
      type: AttentionDecoder
      max_len: 250
      layer:
        d_model: 512
        nhead: 8
      num_layers: 8
    dec2proba:
      type: Tunnel
      input_size: [batch_size, length, 512]
      layers:
        - { type: layernorm, args: { elementwise_affine: false } }
        - { type: linear, size: 129, init: { bias: zero } }
    dec_supporter:
      type: GreedyDecoder
      start_token: 1
    sequencece:
      type: CrossEntropyLoss
      reduction: sum
      ignore_index: 0
    -d_kl:         { type: MinusD_KLLoss }
    -d_kl_factor:  { type: Affine, weight: 0.001 }

optimizer:
  type: "$optimizer"     # adamw
  lr: $lr                # 0.001
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8

scheduler:
  type: warmup
  warmup: 200

metrics:
  Perfect accuracy:
    type: perfect
    input: forced
    target: dec_target
    pad_token: $pad_token
  Partial accuracy (greedy decode):
    type: partial
    input: greedy
    target: dec_target
    pad_token: $pad_token
  Partial accuracy (teacher forcing):
    type: partial
    input: forced
    target: dec_target
    pad_token: $pad_token

pre_hooks:
  notice:
    type: notice_alarm
    studyname: "$studyname"
    alarm: { type: count, target: step, step: 2500, start: 2500 }
    end: true
  step_abort:
    type: step_abort
    threshold: 20000
  time_abort:
    type: time_abort
    threshold: 7200

data:
  train:
    type: normal
    seed: $seed
    batch_size: 64
    datasets:
      input:
        type: pkl
        path_list: ["$DATA_DIR/AllPubchem_test_random.pkl"]
      target:
        type: pkl
        path_list: ["$DATA_DIR/AllPubchem_test_canonical.pkl"]

  vals:
    - type: normal
      seed: $seed
      batch_size: 128
      datasets:
        input:
          type: pkl
          path_list: ["$DATA_DIR/AllPubchem_test_random.pkl"]
        target:
          type: pkl
          path_list: ["$DATA_DIR/AllPubchem_test_canonical.pkl"]

training:
  epochs: 1
  steps_per_epoch: 50
  grad_accum: 1
  save_every: 1
  process:
    type: forward
