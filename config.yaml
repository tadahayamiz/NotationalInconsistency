# ============================================
# config.yaml  (Plan-A ready / single-entry process)
# --------------------------------------------
# ・processブロックは残す（ForwardProcess を使用）
# ・入口moduleは "pipeline"（新規 PipelineModule）に一本化
# ・training.train_loop / val_loop / loss_names は従来どおり保持
# ・scripts/train.py系/あなたのtrain.py系の双方で読める形
# ============================================

variables:
  $study: "AllPubchem_test"

model:
  # ここは既存のモデル定義をそのまま置いてください（エンコーダ/デコーダ/埋め込み等）
  # 例:
  # modules:
  #   enc_embedding: { type: EncEmbedding,  ... }
  #   encoder:       { type: Encoder,       ... }
  #   pooler:        { type: Pooler,        ... }
  #   latent2mu:     { type: Affine,        out_dim: 512 }
  #   latent2var:    { type: Affine,        out_dim: 512 }
  #   vae:           { type: VAESampler }
  #   latent2dec:    { type: Affine,        out_dim: 512 }
  #   dec_embedding: { type: DecEmbedding,  ... }
  #   decoder:       { type: Decoder,       ... }
  #   dec2proba:     { type: LinearHead,    ... }
  #   sequencece:    { type: SequenceCE }
  #   -d_kl:         { type: KLDivergence }
  #   -d_kl_factor:  { type: Scale,         factor: 0.001 }
  #   teacherforcer: { type: TeacherForcer }
  #   masker:        { type: PaddingMasker }
  #   dec_supporter: { type: DecodingSupporter }
  #
  # ↓↓↓ 入口を1本化するための“実行器”モジュール（新規追加） ↓↓↓
  modules:
    pipeline:
      type: PipelineModule
      # trainingセクションを参照して二重管理を避ける（対応していない場合は下の内容をコピペでも可）
      train_loop: ${training.train_loop}
      val_loop:   ${training.val_loop}
      loss_names: ${training.loss_names}

training:
  gpuid: 0
  timestamp: "$TIMESTAMP"

  result_dir:
    dirname: "./result/${timestamp}_${study}"
    duplicate: "merge"  # or "new"

  verbose:
    show_tqdm: true
    loglevel:
      stream: "info"
      file:   "debug"

  # 再現性・挙動
  detect_anomaly: false
  deterministic: false
  model_seed: 111

  # 途中から読み込む場合に使用（不要なら空でOK）
  init_weight: {}
  optimizer_init_weight:

  # -------------------------
  # Data Loader（あなたの既存の定義を入れてください）
  # scripts/train.py の update_dataloader_for_epoch() が以下のキーを更新する想定:
  #   training.data.train.datasets.datasets.input.path_list
  #   training.data.train.datasets.datasets.target.path_list
  # -------------------------
  data:
    train:
      type: pkl_pair               # FIXME: あなたの get_dataloader 実装の type 名に合わせてください
      batch_size: 256              # FIXME
      shuffle: true
      # 下は update_dataloader_for_epoch() で差し替えられる想定のパス構造
      datasets:
        type: paired
        datasets:
          input:
            type: pkl
            path_list: "./data/Pubchem_chunk_pro_0_ran.pkl"
          target:
            type: pkl
            path_list: "./data/Pubchem_chunk_pro_0_can.pkl"
    vals:
      # 例: 検証データを複数名前付きで
      valid:
        type: pkl_pair             # FIXME
        batch_size: 256
        shuffle: false
        datasets:
          type: paired
          datasets:
            input:
              type: pkl
              path_list: "./data/Pubchem_chunk_pro_0_ran.pkl"
            target:
              type: pkl
              path_list: "./data/Pubchem_chunk_pro_0_can.pkl"

  # -------------------------
  # Optimizer / Scheduler
  # -------------------------
  optimizer:
    type: adamw
    lr: 1.0e-3
    weight_decay: 0.0
    betas: [0.9, 0.999]

  schedule:
    opt_freq: 1
  scheduler:
    type: warmup
    warmup: 800
    last_epoch: -1

  # -------------------------
  # ループ定義（従来どおり保持）
  # -------------------------
  train_loop:
    - { module: teacherforcer,  input: target,                                    output: [dec_input, dec_target] }
    - { module: masker,         input: input,                                     output: input_padding_mask }
    - { module: enc_embedding,  input: input,                                     output: input_emb }
    - { module: encoder,        input: [input_emb, input_padding_mask],           output: memory }
    - { type: function, function: { type: transpose, dim0: 0, ...1: 1 }, input: input_padding_mask, output: input_padding_mask2 }
    - { module: pooler,         input: [memory, input_padding_mask2],             output: latent_base }
    - { module: latent2mu,      input: latent_base,                               output: mu }
    - { module: latent2var,     input: latent_base,                               output: var }
    - { module: vae,            input: { mu: mu, var: var },                      output: latent }
    - { module: latent2dec,     input: latent,                                    output: dec_latent }
    - { module: dec_embedding,  input: dec_input }
    - { module: decoder,        input: { latent: dec_latent, tgt: dec_input },    output: dec_output, mode: forced }
    - { module: dec2proba,      input: dec_output }
    - { module: sequencece,     input: [dec_output, dec_target],                  output: sequencece }
    - { module: -d_kl,          input: [mu, var],                                 output: -d_kl }
    - { module: -d_kl_factor,   input: -d_kl }

  loss_names: [sequencece, -d_kl]

  val_loop:
    - { module: teacherforcer,  input: target, output: [dec_input, dec_target, dec_target_len], return_len: true }
    - { module: masker,         input: input,  output: input_padding_mask }
    - { module: enc_embedding,  input: input }
    - { module: encoder,        input: [input, input_padding_mask],               output: memory }
    - { type: function, function: { type: transpose, dim0: 0, ...1: 1 }, input: input_padding_mask, output: input_padding_mask2 }
    - { module: pooler,         input: [memory, input_padding_mask2],             output: latent_base }
    - { module: latent2mu,      input: latent_base,                               output: mu }
    - { module: latent2var,     input: latent_base,                               output: var }
    - { module: vae,            input: { mu: mu, var: var },                      output: latent }
    - { module: latent2dec,     input: latent,                                    output: dec_latent }
    - { module: dec_embedding,  input: dec_input }
    - { module: decoder,        input: { latent: dec_latent, tgt: dec_input },    output: dec_output, mode: forced }
    - { module: dec2proba,      input: dec_output }
    - { module: sequencece,     input: [dec_output, dec_target],                  output: sequencece }
    - { module: -d_kl,          input: [mu, var],                                 output: -d_kl }
    - { module: dec_supporter,  mode: force,                                      input: dec_output,   output: forced }
    - { module: decoder,        mode: prepare_cell_forward,                       input: { latent: dec_latent }, output: state }
    - { module: dec_supporter,  mode: init,                                       input: { batch_size: batch_size }, output: [cur_input, greedy] }
    - type: iterate
      length: dec_target_len
      processes:
        - { module: dec_embedding, input: { input: cur_input, position: iterate_i },                            output: cur_input }
        - { module: decoder,       mode: cell_forward, input: { tgt: cur_input, state: state, position: iterate_i }, output: [cur_output, state] }
        - { module: dec2proba,     input: cur_output }
        - { module: dec_supporter, mode: add,         input: { cur_proba: cur_output, outs: greedy },           output: [cur_input, greedy] }
    - { module: dec_supporter, input: greedy, output: greedy, mode: aggregate }

  # -------------------------
  # Metrics / Accumulators
  # -------------------------
  metrics:
    "Perfect accuracy":
      type: perfect_accuracy      # FIXME: あなたの notate.training.get_metric に合わせる
    "Partial accuracy (greedy decode)":
      type: partial_accuracy_greedy
    "Partial accuracy (teacher forcing)":
      type: partial_accuracy_tf

  accumulators:
    - { type: NumpyAccumulator, input: 'idx', org_type: 'numpy' }  # 例

  stocks:
    score_df: ""   # 既存の val_score.csv を読む場合のみパス

  # -------------------------
  # Hook群（必要に応じて）
  # -------------------------
  pre_hooks:
    notice_alarm:
      type: notice_alarm
      studyname: "${study}"
      alarm: { type: count, target: step, step: 2500, start: 2500 }
      end: true
    save_alarm:
      type: save_alarm
      alarm: { type: list, target: step, list: [0, 1000, 2000, 3000, 4000, 5000] }
      end: true
    time_abort:
      type: time_abort
      threshold: 7200
    step_abort:
      type: step_abort
      threshold: 50000
    scheduler_alarm:
      type: scheduler_alarm
      scheduler:
        type: warmup
        warmup: 800
  post_hooks:
    validation_alarm:
      type: validation_alarm
    checkpoint_alarm:
      type: checkpoint_alarm

  # 旧式の中断パラメータ（Hook化が推奨されるが下位互換で残す）
  abortion:
    step:
    epoch:
    time:

  # ロス正則化・勾配
  regularize_loss:
    normalize: false
    clip_grad_norm:
    clip_grad_value:

  # 乱数状態の保存/復元（使う場合のみ）
  rstate:
    random:
    numpy:
    torch:
    cuda:

# -------------------------
# process（入口1本化：ForwardProcess用）
# -------------------------
process:
  type: forward
  module: pipeline     # ← 上で定義した PipelineModule を入口に
  input: batch         # ← バッチ辞書をそのまま渡す（ForwardProcess 実装に合わせて dict/kwargs 両対応）
