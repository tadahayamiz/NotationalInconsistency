# ============================================================
# config.yaml (runner-side hardening; workers untouched)
# ============================================================

# ---------- Preflight (runner gate; architecture-neutral) ----------
preflight:
  enable: true
  max_length: 198
  max_drop_pct: 5
  check_files: true
  report_dir: ./qc
  apply_to_train: true
  apply_to_vals:  true

# ---------- Model (Back to the original architecture) ----------
model:
  modules:
    teacherforcer:
      type: TeacherForcer
      length_dim: 1

    masker:
      type: MaskMaker
      mask_token: 0
      dtype: bool
      direction: equal

    enc_embedding:
      type: PositionalEmbedding
      embedding:
        num_embeddings: 1123
        embedding_dim: 512
        padding_idx: 0
      dropout: 0.0
      max_len: 250

    encoder:
      type: TransformerEncoder
      layer:
        d_model: 512
        nhead: 8
        d_ff_factor: 4
        dropout: 0.0
        activation: newgelu
        layer_norm_eps: 1.0e-09
      n_layer: 8
      init:
        self_attn.in_proj_weight: glorot_uniform
        self_attn.in_proj_bias: zero
        self_attn.out_proj.weight: glorot_uniform
        self_attn.out_proj.bias: zero
        linear1.weight: {type: normal, mean: 0.0, std: 0.02}
        linear1.bias: zero
        linear2.weight: {type: normal, mean: 0.0, std: 0.02}
        linear2.bias: zero

    pooler:
      type: NoAffinePooler
      input_size: [max_len, batch_size, 512]

    latent2mu:
      type: Tunnel
      input_size: [batch_size, 1536]
      layers:
        - type: linear
          size: 512
          init: {bias: zero}

    latent2var:
      type: Tunnel
      input_size: [batch_size, 1536]
      layers:
        - type: linear
          size: 512
          init: {bias: zero}
        - type: function
          function: {type: softplus}

    vae:
      type: VAE

    latent2dec:
      type: Tunnel
      input_size: [batch_size, 512]
      layers:
        - type: affine
          weight: 10.0
          bias: 0.0
        - type: linear
          size: 512
          init: {bias: zero}

    dec_embedding:
      type: PositionalEmbedding
      embedding:
        num_embeddings: 129
        embedding_dim: 512
        padding_idx: 0
      dropout: 0.0
      max_len: 250

    decoder:
      type: AttentionDecoder
      max_len: 250
      layer:
        d_model: 512
        nhead: 8
        dropout: 0.0
        layer_norm_eps: 1.0e-09
        activation: newgelu
        d_ff_factor: 4           # or dim_feedforward: 2048 (one of them)
      num_layers: 8
      init:
        self_attn.in_proj_weight: glorot_uniform
        self_attn.in_proj_bias: zero
        self_attn.out_proj.weight: glorot_uniform
        self_attn.out_proj.bias: zero
        linear1.weight: {type: normal, mean: 0.0, std: 0.02}
        linear1.bias: zero
        linear2.weight: {type: normal, mean: 0.0, std: 0.02}
        linear2.bias: zero

    dec2proba:
      type: Tunnel
      input_size: [batch_size, length, 512]
      layers:
        - type: layernorm
          args: {elementwise_affine: false}
        - type: linear
          size: 129
          init: {bias: zero}

    dec_supporter:
      type: GreedyDecoder
      start_token: 1

    sequencece:
      type: CrossEntropyLoss
      reduction: sum
      ignore_index: 0

    -d_kl:
      type: MinusD_KLLoss

    -d_kl_factor:
      type: Affine
      weight: 0.001

    transpose_mask:
      type: Tunnel
      input_size: [max_len, batch_size]      # もともとの mask 形状 [T,B] を想定
      layers:
        - type: function
          function:
            type: transpose
            dim0: 0
            dim1: 1                           # => [B, T] へ転置

  # （use_modules は明示不要ですが、残しても実害はありません）
  use_modules:
    - teacherforcer
    - masker
    - enc_embedding
    - encoder
    - pooler
    - latent2mu
    - latent2var
    - vae
    - latent2dec
    - dec_embedding
    - decoder
    - dec2proba
    - dec_supporter
    - sequencece
    - -d_kl
    - -d_kl_factor
    - transpose_mask

  init:
    type: default
  seed: 111

# ---------- Training ----------
training:
  result_dir:
    dirname: "./result/$TIMESTAMP_AllPubchem"
    duplicate: "error"

  verbose:
    show_tqdm: true
    loglevel:
      stream: info
      file: debug

  gpuid: 0
  detect_anomaly: false
  deterministic: false
  runner_seed: 111
  model_seed: 111

  data:
    train:
      type: bucket
      seed: 1
      bucket_dset: input
      batch_size: [128, 96, 64, 48, 32, 16]
      bins: [20, 40, 60, 80, 100, 120, 140, 160]
      add_lower_margin: true
      add_upper_margin: true
      datasets:
        dfs: {}
        datasets:
          input:
            type: string
            dim: 1
            dtype: long
            len_name: src_len
            padding_value: 0
            path_list: "./data/AllPubchem_test_random.pkl"
          target:
            type: string
            dim: 1
            dtype: long
            len_name: tgt_len
            padding_value: 0
            path_list: "./data/AllPubchem_test_canonical.pkl"

    vals:
      valid:
        type: bucket
        seed: 1
        bucket_dset: input
        batch_size: [128, 96, 64, 48, 32, 16]
        bins: [20, 40, 60, 80, 100, 120, 140, 160]
        add_lower_margin: true
        add_upper_margin: true
        datasets:
          dfs: {}
          datasets:
            input:
              type: string
              dim: 1
              dtype: long
              len_name: src_len
              padding_value: 0
              path_list: "./data/AllPubchem_test_random.pkl"
            target:
              type: string
              dim: 1
              dtype: long
              len_name: tgt_len
              padding_value: 0
              path_list: "./data/AllPubchem_test_canonical.pkl"

  optimizer:
    type: adamw
    lr: 0.001
    betas: [0.9, 0.999]
    eps: 1.0e-8
    weight_decay: 0.01

  scheduler:
    type: warmup
    warmup: 800

  schedule:
    opt_freq: 1

  metrics:
    Perfect accuracy:
      type: perfect
      input: forced
      target: dec_target
      pad_token: 0
    Partial accuracy (greedy decode):
      type: partial
      input: greedy
      target: dec_target
      pad_token: 0
    Partial accuracy (teacher forcing):
      type: partial
      input: forced
      target: dec_target
      pad_token: 0

  accumulators:
    - type: numpy
      input: latent
      org_type: torch.tensor
      batch_dim: 0
    - type: numpy
      input: mu
      org_type: torch.tensor
      batch_dim: 0
    - type: numpy
      input: var
      org_type: torch.tensor
      batch_dim: 0

  # hooks は runner 側実装（train.py）に依存するため簡素化を維持
  pre_hooks:
    lr_scheduler:
      type: scheduler_alarm
      target: step
      every: 1
      scheduler: {type: warmup, warmup: 800}

  post_hooks:
    validate:
      type: validation_alarm
      target: step
      every: 1000
    checkpoint:
      type: checkpoint_alarm
      target: step
      every: 5000

  # === Loops: 元の流れに戻す（prepare_cell_forward / force を復元） ===
  train_loop:
    - module: teacherforcer
      input: target
      output: [dec_input, dec_target]

    - module: masker
      input: input
      output: input_padding_mask

    - module: transpose_mask
      input: input_padding_mask
      output: input_padding_mask2

    - module: enc_embedding
      input: input
      output: input_emb

    - module: encoder
      input: [input_emb, input_padding_mask]
      output: memory

    - module: pooler
      input: [memory, input_padding_mask2]
      output: latent_base

    - module: latent2mu
      input: latent_base
      output: mu

    - module: latent2var
      input: latent_base
      output: var

    - module: vae
      input: {mu: mu, var: var}
      output: latent

    - module: latent2dec
      input: latent
      output: dec_latent

    - module: dec_embedding
      input: dec_input

    - module: decoder
      mode: forced
      input: {latent: dec_latent, tgt: dec_input}
      output: dec_output

    - module: dec2proba
      input: dec_output

    - module: sequencece
      input: [dec_output, dec_target]
      output: sequencece

    - module: -d_kl
      input: [mu, var]
      output: -d_kl

    - module: -d_kl_factor
      input: -d_kl

  val_loop:
    - module: teacherforcer
      input: target
      output: [dec_input, dec_target, dec_target_len]
      return_len: true

    - module: masker
      input: input
      output: input_padding_mask

    - module: enc_embedding
      input: input

    - module: encoder
      input: [input, input_padding_mask]
      output: memory

    - module: transpose_mask
      input: input_padding_mask
      output: input_padding_mask2

    - module: pooler
      input: [memory, input_padding_mask2]
      output: latent_base

    - module: latent2mu
      input: latent_base
      output: mu

    - module: latent2var
      input: latent_base
      output: var

    - module: vae
      input: {mu: mu, var: var}
      output: latent

    - module: latent2dec
      input: latent
      output: dec_latent

    # 教師強制での出力（partial_teacher / perfect 用）
    - module: dec_embedding
      input: dec_input
    - module: decoder
      mode: forced
      input: {latent: dec_latent, tgt: dec_input}
      output: dec_output
    - module: dec2proba
      input: dec_output
    - module: sequencece
      input: [dec_output, dec_target]
      output: sequencece
    - module: dec_supporter
      mode: force
      input: dec_output
      output: forced

    # greedy 生成準備
    - module: decoder
      mode: prepare_cell_forward
      input: {latent: dec_latent}
      output: state
    - module: dec_supporter
      mode: init
      input: {batch_size: batch_size}
      output: [cur_input, greedy]

    # greedy 生成ループ
    - type: iterate
      length: dec_target_len
      processes:
        - module: dec_embedding
          input: {input: cur_input, position: iterate_i}
          output: cur_input
        - module: decoder
          mode: cell_forward
          input: {tgt: cur_input, latent: dec_latent, state: state, position: iterate_i}
          output: [cur_output, state]
        - module: dec2proba
          input: cur_output
        - module: dec_supporter
          mode: add
          input: {cur_proba: cur_output, outs: greedy}
          output: [cur_input, greedy]

    - module: dec_supporter
      mode: aggregate
      input: greedy
      output: greedy

  loss_names: ["sequencece", "-d_kl"]
  val_loop_add_train: false
  abortion: { step: null, epoch: null, time: null }
  rstate: {}
  stocks: { score_df: "" }
  steps_per_epoch: 1000
