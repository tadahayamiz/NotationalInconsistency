# ============================================================
# config.yaml (runner-side hardening; workers untouched)
# - Preflight を train/val に適用可能
# - モジュール引数は実装の __init__ シグネチャに揃え済み
# ============================================================

# ---------- Preflight ----------
preflight:
  enable: true
  max_length: 198
  max_drop_pct: 5
  check_files: true
  report_dir: ./qc
  apply_to_train: true
  apply_to_vals:  true

# ---------- Model ----------
model:
  modules:
    teacherforcer:
      type: TeacherForcer
      length_dim: 1          # ← 必須

    masker:
      type: MaskMaker
      mask_token: 3          # ← 語彙の [MASK] トークンIDに合わせて変更

    enc_embedding:
      type: PositionalEmbedding
      embedding:             # ← nn.Embedding の純 kwargs（type は書かない）
        num_embeddings: 400  # (= vocab size)
        embedding_dim: 512   # (= d_model)
      max_len: 256
      dropout: 0.0

    encoder:
      type: TransformerEncoder
      layer:                 # ← SelfAttentionLayer の純 kwargs（type は書かない）
        d_model: 512
        nhead: 8
        dropout: 0.1
      n_layer: 6

    pooler:
      type: MeanPooler

    latent2mu:
      type: MLP
      n_embd: 512           # ← この実装の MLP は n_embd のみ受理

    latent2var:
      type: MLP
      n_embd: 512

    vae:
      type: VAE

    latent2dec:
      type: MLP
      n_embd: 512

    dec_embedding:
      type: PositionalEmbedding
      embedding:
        num_embeddings: 400
        embedding_dim: 512
      max_len: 256
      dropout: 0.0

    decoder:
      type: TransformerDecoder
      layer:
        d_model: 512
        nhead: 8
        dropout: 0.1
      n_layer: 6
      # max_len は実装によって unknown の可能性があるため外しています

    dec2proba:
      type: MLP
      n_embd: 512

    dec_supporter:
      type: GreedyDecoder
      start_token: 1
      end_token: 2

  use_modules:
    - teacherforcer
    - masker
    - enc_embedding
    - encoder
    - pooler
    - latent2mu
    - latent2var
    - vae
    - latent2dec
    - dec_embedding
    - decoder
    - dec2proba
    - dec_supporter

  init:
    type: default
  seed: 111

# ---------- Training ----------
training:
  result_dir:
    dirname: "./result/$TIMESTAMP_AllPubchem"
    duplicate: "error"

  verbose:
    show_tqdm: true
    loglevel:
      stream: info
      file: debug

  gpuid: 0
  detect_anomaly: false
  deterministic: false
  runner_seed: 111
  model_seed: 111

  data:
    train:
      type: bucket
      seed: 1
      bucket_dset: input
      batch_size: [128, 96, 64, 48, 32, 16]
      bins: [20, 40, 60, 80, 100, 120, 140, 160]
      add_lower_margin: true
      add_upper_margin: true
      datasets:
        dfs: {}
        datasets:
          input:
            type: string
            dim: 1
            dtype: long
            len_name: src_len
            padding_value: 0
            path_list: "./data/AllPubchem_test_random.pkl"
          target:
            type: string
            dim: 1
            dtype: long
            len_name: tgt_len
            padding_value: 0
            path_list: "./data/AllPubchem_test_canonical.pkl"

    vals:
      valid:
        type: bucket
        seed: 1
        bucket_dset: input
        batch_size: [128, 96, 64, 48, 32, 16]
        bins: [20, 40, 60, 80, 100, 120, 140, 160]
        add_lower_margin: true
        add_upper_margin: true
        datasets:
          dfs: {}
          datasets:
            input:
              type: string
              dim: 1
              dtype: long
              len_name: src_len
              padding_value: 0
              path_list: "./data/AllPubchem_test_random.pkl"
            target:
              type: string
              dim: 1
              dtype: long
              len_name: tgt_len
              padding_value: 0
              path_list: "./data/AllPubchem_test_canonical.pkl"

  optimizer:
    type: adamw
    lr: 0.001
    betas: [0.9, 0.999]
    eps: 1.0e-8
    weight_decay: 0.01

  scheduler:
    type: warmup
    warmup: 800

  schedule:
    opt_freq: 1

  metrics:
    perfect:
      type: perfect
    partial_teacher:
      type: partial_teacher
    partial_greedy:
      type: partial_greedy

  accumulators:
    - type: numpy
      name: latent
      save_dir: "./result_np/latent"
      fields: ["z", "mu", "var"]
    - type: numpy
      name: pred
      save_dir: "./result_np/pred"
      fields: ["y_pred", "y_true"]

  pre_hooks:
    lr_scheduler:
      type: scheduler_alarm
      target: step
      every: 1
      scheduler:
        type: warmup
        warmup: 800

  post_hooks:
    validate:
      type: validation_alarm
      target: step
      every: 1000
    checkpoint:
      type: checkpoint_alarm
      target: step
      every: 5000

  train_loop:
    - op: "forward"
      inputs: ["src", "tgt"]
      outputs: ["logits"]
    - op: "loss"
      inputs: ["logits", "tgt"]
      outputs: ["loss"]
    - op: "backward"
      inputs: ["loss"]
    - op: "step"
    - op: "metrics"
      inputs: ["logits", "tgt"]
    - op: "accumulate"
      inputs: ["z", "mu", "var", "logits", "tgt"]

  val_loop:
    - op: "forward"
      inputs: ["src", "tgt"]
      outputs: ["logits"]
    - op: "metrics"
      inputs: ["logits", "tgt"]
    - op: "accumulate"
      inputs: ["z", "mu", "var", "logits", "tgt"]

  loss_names: ["loss"]
  val_loop_add_train: false
  abortion: { step: null, epoch: null, time: null }
  rstate: {}
  stocks: { score_df: "" }
  steps_per_epoch: 1000
