# ============================================================
# Strict config (fallback OFF / relative paths / extra keys forbidden)
# 基準ディレクトリ: この config.yaml が置かれている場所
# ============================================================

# 出力ディレクトリ（$TIMESTAMP のみ使用可）
result_dir:
  dirname: "./result/$TIMESTAMP_AllPubchem"
  duplicate: "error"   # "error" | "overwrite" | "merge" | "ask"

# 実行デバイス・乱数
device: "cuda"         # "cuda" | "cpu"
seed: 111

# ログ
logging:
  stream_level: "info"
  file_level: "debug"

# ------------------------------------------------------------
# データ設定（相対パス）
#  - ここでは「バケツ化データセット(bucket_dset)＋ローダ」の一般形
#  - {epoch} プレースホルダは data.* 配下の文字列内のみ許可
# ------------------------------------------------------------
data:
  train:
    type: bucket          # ★ 必須: 'normal' or 'bucket'
    seed: 1               # あれば
    bucket_dset: input    # bucket時は基準データセット名
    batch_size: [128, 96, 64, 48, 32, 16]
    bins: [20, 40, 60, 80, 100, 120, 140, 160]
    add_lower_margin: true
    add_upper_margin: true

    # ↓ DataLoader が期待する形（datasets: の中に datasets/dfs の2階層）
    datasets:
      dfs: {}             # CSVを読む場合だけ定義。pickleだけなら空でOK
      datasets:
        input:
          type: string
          padding_value: 0
          path_list: "./data/train/AllPubchem_train_random.pkl"     # src 側
          len_name: "src_len"
          dtype: long
          dim: 1
        target:
          type: string
          padding_value: 0
          path_list: "./data/train/AllPubchem_train_canonical.pkl"  # tgt 側
          len_name: "tgt_len"
          dtype: long
          dim: 1

  valid:
    type: bucket
    seed: 1
    bucket_dset: input
    batch_size: [128, 96, 64, 48, 32, 16]
    bins: [20, 40, 60, 80, 100, 120, 140, 160]
    add_lower_margin: true
    add_upper_margin: true
    datasets:
      dfs: {}
      datasets:
        input:
          type: string
          padding_value: 0
          path_list: "./data/valid/AllPubchem_valid_random.pkl"
          len_name: "src_len"
          dtype: long
          dim: 1
        target:
          type: string
          padding_value: 0
          path_list: "./data/valid/AllPubchem_valid_canonical.pkl"
          len_name: "tgt_len"
          dtype: long
          dim: 1

# ------------------------------------------------------------
# モデル定義（登録済みのモジュール名に合わせてください）
#  - 代表的構成：埋め込み→Encoder/Decoder→確率化→GreedyDecoder
#  - 以前のGreedyDecoder警告はend_token未指定が原因 → 明記必須
# ------------------------------------------------------------
model:
  modules:
    teacherforcer:
      type: "TeacherForcer"        # 例: 既存名に合わせる
      ratio: 1.0
    masker:
      type: "SequenceMasker"

    enc_embedding:
      type: "PositionalEmbedding"
      vocab_size: 400   # 必要ならvocab_jsonに整合
      d_model: 512
      dropout: 0.0

    encoder:
      type: "TransformerEncoder"
      d_model: 512
      nhead: 8
      num_layers: 6
      dropout: 0.1

    pooler:
      type: "Pooler"
      mode: "mean"

    # VAE系を使う場合のみ（使わなければ削除可）
    latent2mu:
      type: "Linear"
      in_features: 512
      out_features: 512
    latent2var:
      type: "Linear"
      in_features: 512
      out_features: 512
    vae:
      type: "GaussianSampler"

    latent2dec:
      type: "Linear"
      in_features: 512
      out_features: 512

    dec_embedding:
      type: "PositionalEmbedding"
      vocab_size: 400
      d_model: 512
      dropout: 0.0

    decoder:
      type: "TransformerDecoder"
      d_model: 512
      nhead: 8
      num_layers: 6
      dropout: 0.1

    dec2proba:
      type: "LinearClassifier"
      in_features: 512
      num_classes: 400
      tied_with: "enc_embedding"   # 重み共有がある実装なら

    dec_supporter:
      type: "GreedyDecoder"
      start_token: 1
      end_token: 2         # ← 明示必須（フォールバック禁止）
      pad_token: 0
      max_len: 256

  # どのモジュールを使うか（順序も重要なら記述順で）
  use_modules:
    - "teacherforcer"
    - "masker"
    - "enc_embedding"
    - "encoder"
    - "pooler"
    - "latent2mu"
    - "latent2var"
    - "vae"
    - "latent2dec"
    - "dec_embedding"
    - "decoder"
    - "dec2proba"
    - "dec_supporter"

  # 初期化やシード
  init:
    type: "default"
  seed: 111

# ------------------------------------------------------------
# オプティマイザ／スケジューラ
# ------------------------------------------------------------
optimizer:
  type: "adamw"        # "adam" | "adamw" | "sgd"（登録済みに合わせて）
  lr: 1.0e-3
  betas: [0.9, 0.999]
  eps: 1.0e-8
  weight_decay: 0.01

scheduler:
  type: "warmup"       # "none" | "warmup" | "cosine"
  warmup: 800          # ← warmup使用時は必須（train.pyで検証）
  # cosine等を使う場合は追加パラメータを明示

# ------------------------------------------------------------
# メトリクス
# ------------------------------------------------------------
metrics:
  - type: "perfect"                 # 既存名称に合わせる
  - type: "partial_teacher"         # teacher forcing での部分一致
  - type: "partial_greedy"          # greedy decode での部分一致

# ------------------------------------------------------------
# アキュムレータ（例：潜在表現・出力をNPY保存）
# ------------------------------------------------------------
accumulators:
  - name: "latent"
    type: "numpy"
    fields: ["z", "mu", "var"]
    save_dir: "./result_np/latent"
  - name: "pred"
    type: "numpy"
    fields: ["y_pred", "y_true"]
    save_dir: "./result_np/pred"

# ------------------------------------------------------------
# フック（任意）：前処理・後処理
# ------------------------------------------------------------
pre_hooks:
  - type: "AlarmHook"
    target: "step"
    every: 1000
post_hooks:
  - type: "SaverHook"
    every: 1

# ------------------------------------------------------------
# トレーニング設定
# ------------------------------------------------------------
training:
  epochs: 50
  steps_per_epoch: 1000
  grad_accum: 1
  save_every: 1
  verbose: true
  regularize_loss: false

  # ループ構成（既存の命名に合わせてください）
  train_loop:
    - op: "forward"
      inputs: ["src", "tgt"]
      outputs: ["logits"]
    - op: "loss"
      inputs: ["logits", "tgt"]
      outputs: ["loss"]
    - op: "backward"
      inputs: ["loss"]
    - op: "step"
    - op: "metrics"
      inputs: ["logits", "tgt"]
    - op: "accumulate"
      inputs: ["z", "mu", "var", "logits", "tgt"]

  val_loop:
    - op: "forward"
      inputs: ["src", "tgt"]
      outputs: ["logits"]
    - op: "metrics"
      inputs: ["logits", "tgt"]
    - op: "accumulate"
      inputs: ["z", "mu", "var", "logits", "tgt"]

