# ==== config.yaml (lowercased module types; small-data training) ====

variables:
  $studyname: AllPubchem_test
  $seed: 111
  $lr: 0.001
  $optimizer: adamw
  $pad_token: 0
  $start_token: 1
  $end_token: 2
  $voc_size: 129
  $PROJECT_DIR: .
  $DATA_DIR: data

logging:
  stream_level: info
  file_level: debug

result_dir:
  dirname: "./result/$TIMESTAMP_$studyname"
  duplicate: "merge"

model:
  modules:
    teacherforcer: { type: teacherforcer, length_dim: 1 }
    masker:        { type: maskmaker, mask_token: 0, dtype: bool, direction: equal }
    enc_embedding:
      type: positionalembedding
      embedding: { num_embeddings: 1123, embedding_dim: 512, padding_idx: 0 }
      dropout: 0.0
      max_len: 250
    encoder:
      type: transformerencoder
      layer:
        d_model: 512
        nhead: 8
        d_ff_factor: 4
        dropout: 0.0
        activation: newgelu
        layer_norm_eps: 1.0e-09
      n_layer: 8
      init:
        self_attn.in_proj_weight: glorot_uniform
        self_attn.in_proj_bias: zero
        self_attn.out_proj.weight: glorot_uniform
        self_attn.out_proj.bias: zero
        linear1.weight: { type: normal, mean: 0.0, std: 0.02 }
        linear1.bias: zero
        linear2.weight: { type: normal, mean: 0.0, std: 0.02 }
        linear2.bias: zero
    pooler:
      type: noaffinepooler
      input_size: [max_len, batch_size, 512]
    latent2mu:
      type: tunnel
      input_size: [batch_size, 1536]
      layers:
        - { type: linear, size: 512, init: { bias: zero } }
    latent2var:
      type: tunnel
      input_size: [batch_size, 1536]
      layers:
        - { type: linear, size: 512, init: { bias: zero } }
        - { type: function, function: { type: softplus } }
    vae: { type: vae }
    latent2dec:
      type: tunnel
      input_size: [batch_size, 512]
      layers:
        - { type: affine, weight: 10.0, bias: 0.0 }
        - { type: linear, size: 512, init: { bias: zero } }
    dec_embedding:
      type: positionalembedding
      embedding: { num_embeddings: 129, embedding_dim: 512, padding_idx: 0 }
      dropout: 0.0
      max_len: 250
    decoder:
      type: attentiondecoder
      max_len: 250
      layer:
        d_model: 512
        nhead: 8
        dropout: 0.0
        layer_norm_eps: 1.0e-09
        activation: newgelu
        d_ff_factor: 4
      num_layers: 8
      init:
        self_attn.in_proj_weight: glorot_uniform
        self_attn.in_proj_bias: zero
        self_attn.out_proj.weight: glorot_uniform
        self_attn.out_proj.bias: zero
        linear1.weight: { type: normal, mean: 0.0, std: 0.02 }
        linear1.bias: zero
        linear2.weight: { type: normal, mean: 0.0, std: 0.02 }
        linear2.bias: zero
    dec2proba:
      type: tunnel
      input_size: [batch_size, length, 512]
      layers:
        - { type: layernorm, args: { elementwise_affine: false } }
        - { type: linear, size: 129, init: { bias: zero } }
    dec_supporter:
      type: greedydecoder
      start_token: 1
    sequencece:
      type: crossentropyloss
      reduction: sum
      ignore_index: 0
    -d_kl:         { type: minusd_klloss }
    -d_kl_factor:  { type: affine, weight: 0.001 }

optimizer:
  type: "$optimizer"         # adamw
  lr: $lr                    # 0.001
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8

scheduler:
  type: warmup
  warmup: 200

metrics:
  Perfect accuracy:
    type: perfect
    input: forced
    target: dec_target
    pad_token: $pad_token
  Partial accuracy (greedy decode):
    type: partial
    input: greedy
    target: dec_target
    pad_token: $pad_token
  Partial accuracy (teacher forcing):
    type: partial
    input: forced
    target: dec_target
    pad_token: $pad_token

pre_hooks:
  notice:
    type: notice_alarm
    studyname: "$studyname"
    alarm: { type: count, target: step, step: 2500, start: 2500 }
    end: true
  step_abort:
    type: step_abort
    threshold: 20000
  time_abort:
    type: time_abort
    threshold: 7200

data:
  train:
    type: normal
    seed: $seed
    batch_size: 64
    datasets:
      input:
        type: pkl
        path_list:
          - "$DATA_DIR/AllPubchem_test_random.pkl"
      target:
        type: pkl
        path_list:
          - "$DATA_DIR/AllPubchem_test_canonical.pkl"

  vals:
    - type: normal
      seed: $seed
      batch_size: 128
      datasets:
        input:
          type: pkl
          path_list:
            - "$DATA_DIR/AllPubchem_test_random.pkl"
        target:
          type: pkl
          path_list:
            - "$DATA_DIR/AllPubchem_test_canonical.pkl"

training:
  epochs: 1
  steps_per_epoch: 50
  grad_accum: 1
  save_every: 1
  process:
    type: forward
